{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3307b886",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16082/4105704244.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccaa48ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = \"/home/jhcip/.google/credentials/google_credentials.json\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04136ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56dc457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/22 21:43:33 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster(\"spark://de-zoomcamp.us-east4-c.c.dtc-de-course-339113.internal:7077\") \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://de-zoomcamp.us-east4-c.c.dtc-de-course-339113.internal:7077\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d68cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1eb1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o88.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:758)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16082/3347993236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_pbp_2020\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://nfl_data_lake_dtc-de-course-339113/raw/pbp_data/2020/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    351\u001b[0m         self._set_opts(mergeSchema=mergeSchema, pathGlobFilter=pathGlobFilter,\n\u001b[1;32m    352\u001b[0m                        recursiveFileLookup=recursiveFileLookup)\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.0.3-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o88.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:758)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.ClassNotFoundException: Class com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "df_pbp_2020 = spark.read.parquet('gs://nfl_data_lake_dtc-de-course-339113/raw/pbp_data/2020/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca5ee99",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pbp_2020' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16082/2425794348.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_pbp_2020\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_pbp_2020' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/22 21:48:23 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/04/22 21:48:23 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:726)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:153)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:258)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:168)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "df_pbp_2020.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649bb4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['play_id',\n",
       " 'game_id',\n",
       " 'old_game_id',\n",
       " 'home_team',\n",
       " 'away_team',\n",
       " 'season_type',\n",
       " 'week',\n",
       " 'posteam',\n",
       " 'posteam_type',\n",
       " 'defteam',\n",
       " 'side_of_field',\n",
       " 'yardline_100',\n",
       " 'game_date',\n",
       " 'quarter_seconds_remaining',\n",
       " 'half_seconds_remaining',\n",
       " 'game_seconds_remaining',\n",
       " 'game_half',\n",
       " 'quarter_end',\n",
       " 'drive',\n",
       " 'sp',\n",
       " 'qtr',\n",
       " 'down',\n",
       " 'goal_to_go',\n",
       " 'time',\n",
       " 'yrdln',\n",
       " 'ydstogo',\n",
       " 'ydsnet',\n",
       " 'desc',\n",
       " 'play_type',\n",
       " 'yards_gained',\n",
       " 'shotgun',\n",
       " 'no_huddle',\n",
       " 'qb_dropback',\n",
       " 'qb_kneel',\n",
       " 'qb_spike',\n",
       " 'qb_scramble',\n",
       " 'pass_length',\n",
       " 'pass_location',\n",
       " 'air_yards',\n",
       " 'yards_after_catch',\n",
       " 'run_location',\n",
       " 'run_gap',\n",
       " 'field_goal_result',\n",
       " 'kick_distance',\n",
       " 'extra_point_result',\n",
       " 'two_point_conv_result',\n",
       " 'home_timeouts_remaining',\n",
       " 'away_timeouts_remaining',\n",
       " 'timeout',\n",
       " 'timeout_team',\n",
       " 'td_team',\n",
       " 'td_player_name',\n",
       " 'td_player_id',\n",
       " 'posteam_timeouts_remaining',\n",
       " 'defteam_timeouts_remaining',\n",
       " 'total_home_score',\n",
       " 'total_away_score',\n",
       " 'posteam_score',\n",
       " 'defteam_score',\n",
       " 'score_differential',\n",
       " 'posteam_score_post',\n",
       " 'defteam_score_post',\n",
       " 'score_differential_post',\n",
       " 'no_score_prob',\n",
       " 'opp_fg_prob',\n",
       " 'opp_safety_prob',\n",
       " 'opp_td_prob',\n",
       " 'fg_prob',\n",
       " 'safety_prob',\n",
       " 'td_prob',\n",
       " 'extra_point_prob',\n",
       " 'two_point_conversion_prob',\n",
       " 'ep',\n",
       " 'epa',\n",
       " 'total_home_epa',\n",
       " 'total_away_epa',\n",
       " 'total_home_rush_epa',\n",
       " 'total_away_rush_epa',\n",
       " 'total_home_pass_epa',\n",
       " 'total_away_pass_epa',\n",
       " 'air_epa',\n",
       " 'yac_epa',\n",
       " 'comp_air_epa',\n",
       " 'comp_yac_epa',\n",
       " 'total_home_comp_air_epa',\n",
       " 'total_away_comp_air_epa',\n",
       " 'total_home_comp_yac_epa',\n",
       " 'total_away_comp_yac_epa',\n",
       " 'total_home_raw_air_epa',\n",
       " 'total_away_raw_air_epa',\n",
       " 'total_home_raw_yac_epa',\n",
       " 'total_away_raw_yac_epa',\n",
       " 'wp',\n",
       " 'def_wp',\n",
       " 'home_wp',\n",
       " 'away_wp',\n",
       " 'wpa',\n",
       " 'vegas_wpa',\n",
       " 'vegas_home_wpa',\n",
       " 'home_wp_post',\n",
       " 'away_wp_post',\n",
       " 'vegas_wp',\n",
       " 'vegas_home_wp',\n",
       " 'total_home_rush_wpa',\n",
       " 'total_away_rush_wpa',\n",
       " 'total_home_pass_wpa',\n",
       " 'total_away_pass_wpa',\n",
       " 'air_wpa',\n",
       " 'yac_wpa',\n",
       " 'comp_air_wpa',\n",
       " 'comp_yac_wpa',\n",
       " 'total_home_comp_air_wpa',\n",
       " 'total_away_comp_air_wpa',\n",
       " 'total_home_comp_yac_wpa',\n",
       " 'total_away_comp_yac_wpa',\n",
       " 'total_home_raw_air_wpa',\n",
       " 'total_away_raw_air_wpa',\n",
       " 'total_home_raw_yac_wpa',\n",
       " 'total_away_raw_yac_wpa',\n",
       " 'punt_blocked',\n",
       " 'first_down_rush',\n",
       " 'first_down_pass',\n",
       " 'first_down_penalty',\n",
       " 'third_down_converted',\n",
       " 'third_down_failed',\n",
       " 'fourth_down_converted',\n",
       " 'fourth_down_failed',\n",
       " 'incomplete_pass',\n",
       " 'touchback',\n",
       " 'interception',\n",
       " 'punt_inside_twenty',\n",
       " 'punt_in_endzone',\n",
       " 'punt_out_of_bounds',\n",
       " 'punt_downed',\n",
       " 'punt_fair_catch',\n",
       " 'kickoff_inside_twenty',\n",
       " 'kickoff_in_endzone',\n",
       " 'kickoff_out_of_bounds',\n",
       " 'kickoff_downed',\n",
       " 'kickoff_fair_catch',\n",
       " 'fumble_forced',\n",
       " 'fumble_not_forced',\n",
       " 'fumble_out_of_bounds',\n",
       " 'solo_tackle',\n",
       " 'safety',\n",
       " 'penalty',\n",
       " 'tackled_for_loss',\n",
       " 'fumble_lost',\n",
       " 'own_kickoff_recovery',\n",
       " 'own_kickoff_recovery_td',\n",
       " 'qb_hit',\n",
       " 'rush_attempt',\n",
       " 'pass_attempt',\n",
       " 'sack',\n",
       " 'touchdown',\n",
       " 'pass_touchdown',\n",
       " 'rush_touchdown',\n",
       " 'return_touchdown',\n",
       " 'extra_point_attempt',\n",
       " 'two_point_attempt',\n",
       " 'field_goal_attempt',\n",
       " 'kickoff_attempt',\n",
       " 'punt_attempt',\n",
       " 'fumble',\n",
       " 'complete_pass',\n",
       " 'assist_tackle',\n",
       " 'lateral_reception',\n",
       " 'lateral_rush',\n",
       " 'lateral_return',\n",
       " 'lateral_recovery',\n",
       " 'passer_player_id',\n",
       " 'passer_player_name',\n",
       " 'passing_yards',\n",
       " 'receiver_player_id',\n",
       " 'receiver_player_name',\n",
       " 'receiving_yards',\n",
       " 'rusher_player_id',\n",
       " 'rusher_player_name',\n",
       " 'rushing_yards',\n",
       " 'lateral_receiver_player_id',\n",
       " 'lateral_receiver_player_name',\n",
       " 'lateral_receiving_yards',\n",
       " 'lateral_rusher_player_id',\n",
       " 'lateral_rusher_player_name',\n",
       " 'lateral_rushing_yards',\n",
       " 'lateral_sack_player_id',\n",
       " 'lateral_sack_player_name',\n",
       " 'interception_player_id',\n",
       " 'interception_player_name',\n",
       " 'lateral_interception_player_id',\n",
       " 'lateral_interception_player_name',\n",
       " 'punt_returner_player_id',\n",
       " 'punt_returner_player_name',\n",
       " 'lateral_punt_returner_player_id',\n",
       " 'lateral_punt_returner_player_name',\n",
       " 'kickoff_returner_player_name',\n",
       " 'kickoff_returner_player_id',\n",
       " 'lateral_kickoff_returner_player_id',\n",
       " 'lateral_kickoff_returner_player_name',\n",
       " 'punter_player_id',\n",
       " 'punter_player_name',\n",
       " 'kicker_player_name',\n",
       " 'kicker_player_id',\n",
       " 'own_kickoff_recovery_player_id',\n",
       " 'own_kickoff_recovery_player_name',\n",
       " 'blocked_player_id',\n",
       " 'blocked_player_name',\n",
       " 'tackle_for_loss_1_player_id',\n",
       " 'tackle_for_loss_1_player_name',\n",
       " 'tackle_for_loss_2_player_id',\n",
       " 'tackle_for_loss_2_player_name',\n",
       " 'qb_hit_1_player_id',\n",
       " 'qb_hit_1_player_name',\n",
       " 'qb_hit_2_player_id',\n",
       " 'qb_hit_2_player_name',\n",
       " 'forced_fumble_player_1_team',\n",
       " 'forced_fumble_player_1_player_id',\n",
       " 'forced_fumble_player_1_player_name',\n",
       " 'forced_fumble_player_2_team',\n",
       " 'forced_fumble_player_2_player_id',\n",
       " 'forced_fumble_player_2_player_name',\n",
       " 'solo_tackle_1_team',\n",
       " 'solo_tackle_2_team',\n",
       " 'solo_tackle_1_player_id',\n",
       " 'solo_tackle_2_player_id',\n",
       " 'solo_tackle_1_player_name',\n",
       " 'solo_tackle_2_player_name',\n",
       " 'assist_tackle_1_player_id',\n",
       " 'assist_tackle_1_player_name',\n",
       " 'assist_tackle_1_team',\n",
       " 'assist_tackle_2_player_id',\n",
       " 'assist_tackle_2_player_name',\n",
       " 'assist_tackle_2_team',\n",
       " 'assist_tackle_3_player_id',\n",
       " 'assist_tackle_3_player_name',\n",
       " 'assist_tackle_3_team',\n",
       " 'assist_tackle_4_player_id',\n",
       " 'assist_tackle_4_player_name',\n",
       " 'assist_tackle_4_team',\n",
       " 'tackle_with_assist',\n",
       " 'tackle_with_assist_1_player_id',\n",
       " 'tackle_with_assist_1_player_name',\n",
       " 'tackle_with_assist_1_team',\n",
       " 'tackle_with_assist_2_player_id',\n",
       " 'tackle_with_assist_2_player_name',\n",
       " 'tackle_with_assist_2_team',\n",
       " 'pass_defense_1_player_id',\n",
       " 'pass_defense_1_player_name',\n",
       " 'pass_defense_2_player_id',\n",
       " 'pass_defense_2_player_name',\n",
       " 'fumbled_1_team',\n",
       " 'fumbled_1_player_id',\n",
       " 'fumbled_1_player_name',\n",
       " 'fumbled_2_player_id',\n",
       " 'fumbled_2_player_name',\n",
       " 'fumbled_2_team',\n",
       " 'fumble_recovery_1_team',\n",
       " 'fumble_recovery_1_yards',\n",
       " 'fumble_recovery_1_player_id',\n",
       " 'fumble_recovery_1_player_name',\n",
       " 'fumble_recovery_2_team',\n",
       " 'fumble_recovery_2_yards',\n",
       " 'fumble_recovery_2_player_id',\n",
       " 'fumble_recovery_2_player_name',\n",
       " 'sack_player_id',\n",
       " 'sack_player_name',\n",
       " 'half_sack_1_player_id',\n",
       " 'half_sack_1_player_name',\n",
       " 'half_sack_2_player_id',\n",
       " 'half_sack_2_player_name',\n",
       " 'return_team',\n",
       " 'return_yards',\n",
       " 'penalty_team',\n",
       " 'penalty_player_id',\n",
       " 'penalty_player_name',\n",
       " 'penalty_yards',\n",
       " 'replay_or_challenge',\n",
       " 'replay_or_challenge_result',\n",
       " 'penalty_type',\n",
       " 'defensive_two_point_attempt',\n",
       " 'defensive_two_point_conv',\n",
       " 'defensive_extra_point_attempt',\n",
       " 'defensive_extra_point_conv',\n",
       " 'safety_player_name',\n",
       " 'safety_player_id',\n",
       " 'season',\n",
       " 'cp',\n",
       " 'cpoe',\n",
       " 'series',\n",
       " 'series_success',\n",
       " 'series_result',\n",
       " 'order_sequence',\n",
       " 'start_time',\n",
       " 'time_of_day',\n",
       " 'stadium',\n",
       " 'weather',\n",
       " 'nfl_api_id',\n",
       " 'play_clock',\n",
       " 'play_deleted',\n",
       " 'play_type_nfl',\n",
       " 'special_teams_play',\n",
       " 'st_play_type',\n",
       " 'end_clock_time',\n",
       " 'end_yard_line',\n",
       " 'fixed_drive',\n",
       " 'fixed_drive_result',\n",
       " 'drive_real_start_time',\n",
       " 'drive_play_count',\n",
       " 'drive_time_of_possession',\n",
       " 'drive_first_downs',\n",
       " 'drive_inside20',\n",
       " 'drive_ended_with_score',\n",
       " 'drive_quarter_start',\n",
       " 'drive_quarter_end',\n",
       " 'drive_yards_penalized',\n",
       " 'drive_start_transition',\n",
       " 'drive_end_transition',\n",
       " 'drive_game_clock_start',\n",
       " 'drive_game_clock_end',\n",
       " 'drive_start_yard_line',\n",
       " 'drive_end_yard_line',\n",
       " 'drive_play_id_started',\n",
       " 'drive_play_id_ended',\n",
       " 'away_score',\n",
       " 'home_score',\n",
       " 'location',\n",
       " 'result',\n",
       " 'total',\n",
       " 'spread_line',\n",
       " 'total_line',\n",
       " 'div_game',\n",
       " 'roof',\n",
       " 'surface',\n",
       " 'temp',\n",
       " 'wind',\n",
       " 'home_coach',\n",
       " 'away_coach',\n",
       " 'stadium_id',\n",
       " 'game_stadium',\n",
       " 'aborted_play',\n",
       " 'success',\n",
       " 'passer',\n",
       " 'passer_jersey_number',\n",
       " 'rusher',\n",
       " 'rusher_jersey_number',\n",
       " 'receiver',\n",
       " 'receiver_jersey_number',\n",
       " 'pass',\n",
       " 'rush',\n",
       " 'first_down',\n",
       " 'special',\n",
       " 'play',\n",
       " 'passer_id',\n",
       " 'rusher_id',\n",
       " 'receiver_id',\n",
       " 'name',\n",
       " 'jersey_number',\n",
       " 'id',\n",
       " 'fantasy_player_name',\n",
       " 'fantasy_player_id',\n",
       " 'fantasy',\n",
       " 'fantasy_id',\n",
       " 'out_of_bounds',\n",
       " 'home_opening_kickoff',\n",
       " 'qb_epa',\n",
       " 'xyac_epa',\n",
       " 'xyac_mean_yardage',\n",
       " 'xyac_median_yardage',\n",
       " 'xyac_success',\n",
       " 'xyac_fd',\n",
       " 'xpass',\n",
       " 'pass_oe']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pbp_2020.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90cd6845",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('data/pq/yellow/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88822efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = df_yellow \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "610167a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_colums = []\n",
    "\n",
    "yellow_columns = set(df_yellow.columns)\n",
    "\n",
    "for col in df_green.columns:\n",
    "    if col in yellow_columns:\n",
    "        common_colums.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0f748a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "839d773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2498810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_sel = df_green \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19032efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_sel = df_yellow \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('yellow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5b0f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_data = df_green_sel.unionAll(df_yellow_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bed8b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|   count|\n",
      "+------------+--------+\n",
      "|       green| 2304517|\n",
      "|      yellow|39649199|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trips_data.groupBy('service_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28cc8fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'congestion_surcharge',\n",
       " 'service_type']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trips_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36e90cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips_data.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0e01bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|service_type|count(1)|\n",
      "+------------+--------+\n",
      "|       green| 2304517|\n",
      "|      yellow|39649199|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    service_type,\n",
    "    count(1)\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY \n",
    "    service_type\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2ee7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Reveneue grouping \n",
    "    PULocationID AS revenue_zone,\n",
    "    date_trunc('month', pickup_datetime) AS revenue_month, \n",
    "    service_type, \n",
    "\n",
    "    -- Revenue calculation \n",
    "    SUM(fare_amount) AS revenue_monthly_fare,\n",
    "    SUM(extra) AS revenue_monthly_extra,\n",
    "    SUM(mta_tax) AS revenue_monthly_mta_tax,\n",
    "    SUM(tip_amount) AS revenue_monthly_tip_amount,\n",
    "    SUM(tolls_amount) AS revenue_monthly_tolls_amount,\n",
    "    SUM(improvement_surcharge) AS revenue_monthly_improvement_surcharge,\n",
    "    SUM(total_amount) AS revenue_monthly_total_amount,\n",
    "    SUM(congestion_surcharge) AS revenue_monthly_congestion_surcharge,\n",
    "\n",
    "    -- Additional calculations\n",
    "    AVG(passenger_count) AS avg_montly_passenger_count,\n",
    "    AVG(trip_distance) AS avg_montly_trip_distance\n",
    "FROM\n",
    "    trips_data\n",
    "GROUP BY\n",
    "    1, 2, 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f67eeb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.coalesce(1).write.parquet('data/report/revenue/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a885d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
